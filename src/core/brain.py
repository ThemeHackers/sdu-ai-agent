import os
import logging
import chromadb
import requests
from chromadb import Documents, EmbeddingFunction, Embeddings
from google import genai
from dotenv import load_dotenv

logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(message)s')
load_dotenv()

class GoogleGenAIEmbeddingFunction(EmbeddingFunction):
    def __init__(self, api_key: str, model_name: str = "models/gemini-embedding-001"):
        self.client = genai.Client(api_key=api_key)
        self.model_name = model_name

    def __call__(self, input: Documents) -> Embeddings:
        embeddings = []
        for text in input:
            try:
                response = self.client.models.embed_content(
                    model=self.model_name,
                    contents=text,
                    config={'task_type': 'RETRIEVAL_DOCUMENT'}
                )
                embeddings.append(response.embeddings[0].values)
            except Exception as e:
                embeddings.append([0.0]*768)
        return embeddings

class OllamaEmbeddingFunction(EmbeddingFunction):
    def __init__(self, base_url: str, model_name: str):
        self.base_url = base_url
        self.model_name = model_name

    def __call__(self, input: Documents) -> Embeddings:
        embeddings = []
        for text in input:
            try:
                response = requests.post(
                    f"{self.base_url}/api/embeddings",
                    json={"model": self.model_name, "prompt": text}
                )
                if response.status_code == 200:
                    embeddings.append(response.json()["embedding"])
                else:
                    embeddings.append([0.0]*768)
            except Exception:
                embeddings.append([0.0]*768)
        return embeddings

class SmartBrain:
    def __init__(self, collection_name: str = "sdu_knowledge_v3"):
        self.provider = os.getenv("LLM_PROVIDER", "gemini").lower()
        self.db_path = "./data/chroma_db_v3"
        self.chroma_client = chromadb.PersistentClient(path=self.db_path)
        
        # Provider Configuration
        if self.provider == "ollama":
            self.ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            self.model_name = os.getenv("OLLAMA_MODEL", "llama3")
            self.embedding_model = os.getenv("OLLAMA_EMBEDDING_MODEL", "nomic-embed-text")
            
            # Use a separate collection for Ollama to avoid embedding conflicts
            if collection_name == "sdu_knowledge_v3":
                collection_name = f"sdu_knowledge_ollama_{self.embedding_model.replace('-', '_')}"
                
            self.client = None # Not used for Ollama directly
            self.ef = OllamaEmbeddingFunction(base_url=self.ollama_base_url, model_name=self.embedding_model)
            print(f"üß† SmartBrain initialized using OLLAMA ({self.model_name})")
            print(f"   Collection: {collection_name}")
            
        else: # Default to Gemini
            self.api_key = os.getenv("GEMINI_API_KEY")
            if self.api_key:
                self.client = genai.Client(api_key=self.api_key)
                self.model_name = "gemini-2.5-flash"
                self.ef = GoogleGenAIEmbeddingFunction(api_key=self.api_key)
                print(f"üß† SmartBrain initialized using GEMINI ({self.model_name})")
            else:
                self.client = None
                self.ef = None
                print("‚ö†Ô∏è Warning: GEMINI_API_KEY not found. SmartBrain is disabled.")

        try:
            if self.ef:
                self.collection = self.chroma_client.get_or_create_collection(
                    name=collection_name, 
                    embedding_function=self.ef
                )
            else:
                self.collection = None
        except Exception as e:
            print(f"‚ùå Error initializing collection: {e}")
            self.collection = None

    def _generate_content(self, system_instruction: str, contents: list, temperature: float = 0.3) -> str:
        """Internal helper to route generation requests."""
        if self.provider == "ollama":
            # Combine system instruction and contents for Ollama
            full_prompt = f"{system_instruction}\n\n"
            for item in contents:
                if isinstance(item, str):
                    full_prompt += item + "\n"
                elif isinstance(item, dict) and "content" in item:
                    full_prompt += item["content"] + "\n"
            
            try:
                response = requests.post(
                    f"{self.ollama_base_url}/api/generate",
                    json={
                        "model": self.model_name,
                        "prompt": full_prompt,
                        "stream": False,
                        "options": {"temperature": temperature}
                    }
                )
                if response.status_code == 200:
                    return response.json().get("response", "")
                return ""
            except Exception as e:
                logging.error(f"Ollama generation error: {e}")
                return ""
        
        elif self.client: # Gemini
            try:
                # Filter contents to be just strings or valid parts for Gemini
                processed_contents = []
                for item in contents:
                     if isinstance(item, str):
                         processed_contents.append(item)
                     elif isinstance(item, dict) and "content" in item:
                         processed_contents.append(item["content"]) # Simplification
                
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=[system_instruction] + processed_contents,
                    config={'temperature': temperature}
                )
                return response.text
            except Exception as e:
                logging.error(f"Gemini generation error: {e}")
                return ""
        return ""

    def expand_query(self, query: str) -> str:
        if not self.ef: return query # No provider
        
        system_prompt = """
        ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Search Expert)
        ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô "‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (Search Query)" ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        
        ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
        - Input: "‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"
        - Output: "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ ‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏™‡∏ß‡∏ô‡∏î‡∏∏‡∏™‡∏¥‡∏ï"
        - Input: "‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°"
        - Output: "‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°‡∏ï‡∏•‡∏≠‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏ï‡∏£‡∏µ"
        
        ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ "‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡πâ‡∏ß" ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏≠‡∏∑‡πà‡∏ô
        """
        response = self._generate_content(system_instruction=system_prompt, contents=[query], temperature=0.3)
        return response.strip() if response else query

    def retrieve(self, query: str, top_k: int = 15) -> list:
        if not self.collection:
            return []

        search_query = self.expand_query(query)

        try:
            results = self.collection.query(
                query_texts=[search_query],
                n_results=top_k
            )
            
            if not results['documents'] or not results['documents'][0]:
                return []

            candidates = []
            for i in range(len(results['documents'][0])):
                candidates.append({
                    "text": results['documents'][0][i],
                    "metadata": results['metadatas'][0][i],
                    "score": results['distances'][0][i] if 'distances' in results else 0
                })
            return candidates
        except Exception:
            return []

    def rerank(self, query: str, candidates: list, top_n: int = 5) -> list:
        if not candidates:
            return []
            
        fragments = ""
        for i, cand in enumerate(candidates):
            fragments += f"[{i}]: {cand['text'][:500]}\n---\n"

        system_rerank_prompt = f"""
        ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ "RAG Ranker" ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡∏ß‡πà‡∏≤‡∏≠‡∏±‡∏ô‡πÑ‡∏´‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°" ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        
        ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}
        
        ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
        {fragments}
        
        ‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à:
        1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        2. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏°‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô {top_n} ‡∏•‡∏≥‡∏î‡∏±‡∏ö
        3. ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô JSON Array ‡∏Ç‡∏≠‡∏á Index ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        
        Output Format:
        [index1, index2, index3]
        
        ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
        [5, 2, 0]
        """

        try:
            response = self._generate_content(system_instruction=system_rerank_prompt, contents=[], temperature=0.0)
            
            # Allow for some cleanup if the model outputs markdown code blocks
            clean_response = response.strip()
            if clean_response.startswith("```json"):
                clean_response = clean_response[7:-3]
            elif clean_response.startswith("```"):
                clean_response = clean_response[3:-3]
            
            import json
            indices = json.loads(clean_response)
            
            if not isinstance(indices, list):
                 logging.warning(f"Rerank output not a list: {response}")
                 return candidates[:top_n]

            reranked = []
            seen_indices = set()
            for idx in indices:
                try:
                    idx = int(idx)
                    if 0 <= idx < len(candidates) and idx not in seen_indices:
                        reranked.append(candidates[idx])
                        seen_indices.add(idx)
                except (ValueError, TypeError):
                    continue
            
            if not reranked:
                 return candidates[:top_n]

            return reranked
            
        except Exception as e:
            logging.error(f"Reranking error: {e}")
            return candidates[:top_n]

    def think(self, query: str, context: str, history: list = None):
        if not self.ef:
             yield "System Error: No LLM Provider configured."
             return

        if not context:
            context = "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏™‡∏ß‡∏ô‡∏î‡∏∏‡∏™‡∏¥‡∏ï‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ"

        system_instruction = """
        ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ "‡∏û‡∏µ‡πà‡∏™‡∏ß‡∏ô‡∏î‡∏∏‡∏™‡∏¥‡∏ï (SDU Smart Senior)" AI ‡∏£‡∏∏‡πà‡∏ô‡∏û‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏™‡∏ß‡∏ô‡∏î‡∏∏‡∏™‡∏¥‡∏ï
        ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ô‡πâ‡∏≠‡∏á‡πÜ ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á

        Personality & Tone:
        - ‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡∏≠‡πà‡∏≠‡∏ô‡πÇ‡∏¢‡∏ô ‡∏Ç‡∏µ‡πâ‡πÄ‡∏•‡πà‡∏ô‡∏ô‡∏¥‡∏î‡πÜ ‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á (‡πÉ‡∏ä‡πâ‡∏™‡∏£‡∏£‡∏û‡∏ô‡∏≤‡∏° "‡∏û‡∏µ‡πà" ‡∏Å‡∏±‡∏ö "‡∏ô‡πâ‡∏≠‡∏á")
        - ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏•‡∏∞‡∏™‡∏•‡∏ß‡∏¢ ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (Semiprofessional)
        - ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏£‡∏∞‡∏ï‡∏∑‡∏≠‡∏£‡∏∑‡∏≠‡∏•‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠

        Strict Guidelines:
        1. **Context First:** ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏¢‡∏∂‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å [Context ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á] ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ß‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÄ‡∏≠‡∏á‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
        2. **Unknown Data:** ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Context ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏†‡∏≤‡∏û‡∏ß‡πà‡∏≤ "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡∏û‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡∏ô‡πâ‡∏≠‡∏á‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏•‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏Ñ‡∏ì‡∏∞/‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö"
        3. **Safety:** ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö ‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á, ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏û‡∏®, ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏¥‡πà‡∏á‡∏ú‡∏¥‡∏î‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢
        4. **Structure:** ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ (‡πÉ‡∏ä‡πâ Bullet points, ‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏≤) ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏¢‡∏≤‡∏ß

        Goal: ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏≠‡∏∏‡πà‡∏ô‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        """
        
        user_message_content = f"[Context ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á]:\n{context}\n\n[‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°]: {query}"
        messages = [{"role": "user", "content": user_message_content}]
        
        if history:
            full_history = []
            for h in history[-4:]: 
                full_history.append({"role": h["role"], "content": h["content"]})
            messages = full_history + messages

        if self.provider == "ollama":
            # Direct generation for Ollama
            full_text = self._generate_content(system_instruction, messages, temperature=0.3)
            # Simulate streaming
            chunk_size = 5
            for i in range(0, len(full_text), chunk_size):
                yield full_text[i:i+chunk_size]
        else:
            # Gemini with True Streaming
            try:
                processed_contents = [system_instruction] + [m["content"] for m in messages]
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=processed_contents,
                    config={
                        'temperature': 0.3,
                        'max_output_tokens': 800
                    },
                    stream=True
                )
                
                for chunk in response:
                    if chunk.text:
                        yield chunk.text
                    
            except Exception as e:
                error_msg = str(e)
                if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg:
                    yield "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏û‡∏µ‡πà‡∏™‡∏ß‡∏ô‡∏î‡∏∏‡∏™‡∏¥‡∏ï‡∏´‡∏°‡∏î‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (Rate Limit Exceeded) ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö"
                else:
                    yield f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏û‡∏µ‡πà‡∏™‡∏ß‡∏ô‡∏î‡∏∏‡∏™‡∏¥‡∏ï‡πÄ‡∏Å‡∏¥‡∏î‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏°‡∏∂‡∏ô‡∏á‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß ({error_msg})"

if __name__ == "__main__":
    brain = SmartBrain()
    q = "‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á ‡∏°.‡∏™‡∏ß‡∏ô‡∏î‡∏∏‡∏™‡∏¥‡∏ï ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£"
    # Testing only
    print("Testing generate...")
    try:
        candidates = brain.retrieve(q, top_k=10)
        reranked = brain.rerank(q, candidates, top_n=3)
        final_context = "\n\n".join([f"[‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å: {c['metadata'].get('source', 'Unknown')}]\n{c['text']}" for c in reranked])
        print("Response:")
        for chunk in brain.think(q, final_context):
            print(chunk, end="", flush=True)
        print()
    except Exception as e:
        print(f"Error in main: {e}")
