
import os
import json
import glob
import time
import random
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.core.ingest import extract_content, recursive_split_text
from dotenv import load_dotenv
from google import genai

load_dotenv()

API_KEY = os.getenv("GEMINI_API_KEY")

if not API_KEY:
    print("Error: GEMINI_API_KEY not found.")
    exit(1)

client = genai.Client(api_key=API_KEY)

def generate_qa_pair(context_text):
    prompt = f"""
    ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö AI (QA Generation Expert)
    
    Context (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á):
    "{context_text}"
    
    ‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à:
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (Question-Answer Pair) ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 1 ‡∏Ç‡πâ‡∏≠ ‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏à‡∏≤‡∏Å Context ‡∏ô‡∏µ‡πâ
    ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
    1. ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏Ñ‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏ñ‡∏≤‡∏° (Natural Language) ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö
    2. ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏° Context ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏≠‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏ï‡∏≠‡∏ö
    3. ‡∏ñ‡πâ‡∏≤ Context ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏Ñ‡πà‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "N/A" ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    
    Output Format (JSON Only):
    {{
        "question": "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢...",
        "answer": "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢..."
    }}
    """
    
    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt,
            config={'response_mime_type': 'application/json'}
        )
        return json.loads(response.text)
    except Exception as e:
        error_msg = str(e)
        if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg:
            print("\nüö® API Quota Exceeded (429). Stopping process immediately.")
            sys.exit(1)
        print(f"Error generating QA: {e}")
        return None

def main():
    data_dir = "data/processed"
    output_file = "data/synthetic_dataset.json"
    
    all_files = glob.glob(os.path.join(data_dir, "*.md"))
    print(f"Found {len(all_files)} files.")
    
    if os.path.exists(output_file):
        try:
            with open(output_file, "r", encoding="utf-8") as f:
                dataset = json.load(f)
            print(f"Loaded {len(dataset)} existing items.")
        except:
             print("Could not load existing dataset, starting fresh.")
             dataset = []
    else:
        dataset = []
    
    existing_contexts = {item.get("context_sample", "")[:50] for item in dataset} # Simple dedup

    for file_path in all_files:
        print(f"Processing {file_path}...")
        results = extract_content(file_path)
        
        for item in results:
            text = item["text"]
            chunks = recursive_split_text(text)
            
            for i, chunk in enumerate(chunks):
                if len(chunk) < 100: 
                    continue
                
                # Check duplication
                if chunk[:50] in existing_contexts:
                     print(f"  Skipping chunk {i} (already exists)...")
                     continue

                # Reconstruct chunk ID to match ingest.py
                # Format: filename_page_sheet_i -> sanitized
                # For md files, page and sheet are empty strings.
                raw_id = f"{os.path.basename(file_path)}___{i}"
                import re
                chunk_id = re.sub(r'[^a-zA-Z0-9_-]', '_', raw_id)
                
                print(f"  Generating QA for chunk {i} (ID: {chunk_id})...")
                qa = generate_qa_pair(chunk)
                
                if qa and qa.get("question") != "N/A" and qa.get("answer") != "N/A":
                    dataset.append({
                        "query": qa["question"],
                        "reference_answer": qa["answer"],
                        "relevant_ids": [chunk_id], 
                        "source_file": os.path.basename(file_path),
                        "context_sample": chunk[:200]
                    })
                    
                    # Save incrementally
                    with open(output_file, "w", encoding="utf-8") as f:
                        json.dump(dataset, f, ensure_ascii=False, indent=2)
                
                time.sleep(2) # Avoid rate limits (increased to 2s)

    print(f"‚úÖ Finished. Total {len(dataset)} QA pairs. Saved to {output_file}")

if __name__ == "__main__":
    main()
